{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Ranking Yu et al. 2017 Model\n",
    "\n",
    "Our goal here to to reimplement Yu et al. 2017 93% relation model. \n",
    "\n",
    "First things first, set up the initial configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.4 (default, Dec 19 2017, 17:29:45) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('Python Version:', sys.version)\n",
    "import pandas as pd\n",
    "import logging\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from lib.utils import setup_training\n",
    "\n",
    "# Create root logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 80)\n",
    "\n",
    "random_seed = 123\n",
    "device = 0\n",
    "is_cuda, _ = setup_training(device, random_seed) \n",
    "# Async minibatch allocation for speed\n",
    "# Reference: http://timdettmers.com/2015/03/09/deep-learning-hardware-guide/\n",
    "# TODO: look into cuda_async device=device\n",
    "cuda_async = lambda t: t.cuda(async=True) if is_cuda else t  # Use with tensors\n",
    "cuda = lambda t: t.cuda(device_id=device) if is_cuda else t  # Use with nn.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Load our dataset. Log a couple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.datasets.dataset import Dataset\n",
    "\n",
    "def yu_dataset(directory='../../data/yu/',\n",
    "               train=False,\n",
    "               dev=False,\n",
    "               test=False,\n",
    "               train_filename='train.replace_ne.withpool',\n",
    "               dev_filename='valid.replace_ne.withpool',\n",
    "               test_filename='test.replace_ne.withpool',\n",
    "               vocab_filename='relation.2M.list'):\n",
    "    \"\"\"\n",
    "    Example line example: 40\t61 40 117\twhich genre of album is #head_entity# ?\n",
    "    Vocab example: /film/film/genre\n",
    "    \n",
    "    Sample Data:\n",
    "        Question: 'which genre of album is #head_entity# ?'\n",
    "        True Relation: '/music/album/genre'\n",
    "        False Relation Pool: ['/music/album/release_type', '/music/album/genre', '/music/album/artist']\n",
    "    \"\"\"\n",
    "    vocab_path = os.path.join(directory, vocab_filename)\n",
    "    vocab = [l.strip() for l in open(vocab_path, 'r')]\n",
    "    \n",
    "    ret = []\n",
    "    datasets = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n",
    "    for is_requested, filename in datasets:\n",
    "        if not is_requested:\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(directory, filename)\n",
    "        data = pd.read_table(file_path, header=None, names=['True Relation', 'Relation Pool', 'Question'])\n",
    "        rows = []\n",
    "        for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "            if row['Relation Pool'].strip() == 'noNegativeAnswer':\n",
    "                continue\n",
    "            relation_pool = [vocab[int(i) - 1] for i in row['Relation Pool'].split()]\n",
    "            true_relation = vocab[int(row['True Relation']) - 1]\n",
    "            question = row['Question'].strip()\n",
    "            # Development and test set may or may not have the True relation based on our predicted pool\n",
    "            if filename == train_filename:\n",
    "                assert true_relation not in relation_pool\n",
    "                \n",
    "            for relation in relation_pool:\n",
    "                if filename == train_filename:\n",
    "                    rows.append({'Question': question,\n",
    "                                 'True Relation': true_relation,\n",
    "                                 'False Relation': relation,\n",
    "                                 'Example ID': i})\n",
    "                else:\n",
    "                    rows.append({'Question': question,\n",
    "                                 'True Relation': true_relation,\n",
    "                                 'Relation': relation,\n",
    "                                 'Example ID': i})\n",
    "        ret.append(Dataset(rows))\n",
    "\n",
    "    if len(ret) == 1:\n",
    "        return ret[0]\n",
    "    else:\n",
    "        return tuple(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72238/72238 [00:05<00:00, 13422.38it/s]\n",
      "100%|██████████| 10309/10309 [00:00<00:00, 13149.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Training Data: 499037\n",
      "Train Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example ID</th>\n",
       "      <th>False Relation</th>\n",
       "      <th>Question</th>\n",
       "      <th>True Relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/book/book/genre</td>\n",
       "      <td>what is the book #head_entity# about</td>\n",
       "      <td>/book/written_work/subjects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>/book/written_work/author</td>\n",
       "      <td>what is the book #head_entity# about</td>\n",
       "      <td>/book/written_work/subjects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>/music/release_track/recording</td>\n",
       "      <td>to what release does the release track #head_entity# come from</td>\n",
       "      <td>/music/release_track/release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>/film/film/genre</td>\n",
       "      <td>what country was the film #head_entity# from</td>\n",
       "      <td>/film/film/country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>/media_common/netflix_title/netflix_genres</td>\n",
       "      <td>what country was the film #head_entity# from</td>\n",
       "      <td>/film/film/country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Example ID                              False Relation                                                        Question                 True Relation\n",
       "0           0                            /book/book/genre                            what is the book #head_entity# about   /book/written_work/subjects\n",
       "1           0                   /book/written_work/author                            what is the book #head_entity# about   /book/written_work/subjects\n",
       "2           1              /music/release_track/recording  to what release does the release track #head_entity# come from  /music/release_track/release\n",
       "3           2                            /film/film/genre                    what country was the film #head_entity# from            /film/film/country\n",
       "4           2  /media_common/netflix_title/netflix_genres                    what country was the film #head_entity# from            /film/film/country"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num Development Data: 80544\n",
      "Development Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Relation</th>\n",
       "      <th>True Relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>who was the #head_entity# named after</td>\n",
       "      <td>/location/location/containedby</td>\n",
       "      <td>/symbols/namesake/named_after</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>who was the #head_entity# named after</td>\n",
       "      <td>/symbols/namesake/named_after</td>\n",
       "      <td>/symbols/namesake/named_after</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>where was #head_entity# born</td>\n",
       "      <td>/sports/professional_sports_team/draft_picks</td>\n",
       "      <td>/people/person/place_of_birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>where was #head_entity# born</td>\n",
       "      <td>/people/person/gender</td>\n",
       "      <td>/people/person/place_of_birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>where was #head_entity# born</td>\n",
       "      <td>/people/person/nationality</td>\n",
       "      <td>/people/person/place_of_birth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Example ID                               Question                                      Relation                  True Relation\n",
       "0           0  who was the #head_entity# named after                /location/location/containedby  /symbols/namesake/named_after\n",
       "1           0  who was the #head_entity# named after                 /symbols/namesake/named_after  /symbols/namesake/named_after\n",
       "2           1           where was #head_entity# born  /sports/professional_sports_team/draft_picks  /people/person/place_of_birth\n",
       "3           1           where was #head_entity# born                         /people/person/gender  /people/person/place_of_birth\n",
       "4           1           where was #head_entity# born                    /people/person/nationality  /people/person/place_of_birth"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "train_dataset, dev_dataset = yu_dataset(train=True, dev=True)\n",
    "\n",
    "print('Num Training Data: %d' % len(train_dataset))\n",
    "print('Train Sample:')\n",
    "display(pd.DataFrame(train_dataset[:5]))\n",
    "print('\\nNum Development Data: %d' % len(dev_dataset))\n",
    "print('Development Sample:')\n",
    "display(pd.DataFrame(dev_dataset[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text\n",
    "\n",
    "Here we encode our data into a numerical format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoder vocab size: 5947\n",
      "Relation word encoder vocab size: 3022\n",
      "Relation encoder vocab size: 4554\n",
      "Train Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example ID</th>\n",
       "      <th>False Relation</th>\n",
       "      <th>False Relation Word</th>\n",
       "      <th>Question</th>\n",
       "      <th>True Relation</th>\n",
       "      <th>True Relation Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[5, 17, 17, 699]</td>\n",
       "      <td>[5, 16, 16, 468]</td>\n",
       "      <td>[1749, 3683, 4717, 2999, 3123, 5634]</td>\n",
       "      <td>[5, 17, 1894, 1837]</td>\n",
       "      <td>[5, 16, 2743, 2726, 1241]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[5, 17, 1894, 2706]</td>\n",
       "      <td>[5, 16, 2743, 2726, 1817]</td>\n",
       "      <td>[1749, 3683, 4717, 2999, 3123, 5634]</td>\n",
       "      <td>[5, 17, 1894, 1837]</td>\n",
       "      <td>[5, 16, 2743, 2726, 1241]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[5, 106, 1561, 1004]</td>\n",
       "      <td>[5, 73, 1242, 2110, 680]</td>\n",
       "      <td>[3121, 1749, 632, 1197, 4717, 632, 2548, 3123, 5066, 1718]</td>\n",
       "      <td>[5, 106, 1561, 1838]</td>\n",
       "      <td>[5, 73, 1242, 2110, 1242]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[5, 4067, 4067, 699]</td>\n",
       "      <td>[5, 2720, 2720, 468]</td>\n",
       "      <td>[1749, 3993, 5606, 4717, 2850, 3123, 1718]</td>\n",
       "      <td>[5, 4067, 4067, 2978]</td>\n",
       "      <td>[5, 2720, 2720, 1999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[5, 4527, 3295, 1026]</td>\n",
       "      <td>[5, 840, 2644, 297, 2431, 297, 2390]</td>\n",
       "      <td>[1749, 3993, 5606, 4717, 2850, 3123, 1718]</td>\n",
       "      <td>[5, 4067, 4067, 2978]</td>\n",
       "      <td>[5, 2720, 2720, 1999]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Example ID         False Relation                   False Relation Word                                                    Question          True Relation         True Relation Word\n",
       "0           0       [5, 17, 17, 699]                      [5, 16, 16, 468]                        [1749, 3683, 4717, 2999, 3123, 5634]    [5, 17, 1894, 1837]  [5, 16, 2743, 2726, 1241]\n",
       "1           0    [5, 17, 1894, 2706]             [5, 16, 2743, 2726, 1817]                        [1749, 3683, 4717, 2999, 3123, 5634]    [5, 17, 1894, 1837]  [5, 16, 2743, 2726, 1241]\n",
       "2           1   [5, 106, 1561, 1004]              [5, 73, 1242, 2110, 680]  [3121, 1749, 632, 1197, 4717, 632, 2548, 3123, 5066, 1718]   [5, 106, 1561, 1838]  [5, 73, 1242, 2110, 1242]\n",
       "3           2   [5, 4067, 4067, 699]                  [5, 2720, 2720, 468]                  [1749, 3993, 5606, 4717, 2850, 3123, 1718]  [5, 4067, 4067, 2978]      [5, 2720, 2720, 1999]\n",
       "4           2  [5, 4527, 3295, 1026]  [5, 840, 2644, 297, 2431, 297, 2390]                  [1749, 3993, 5606, 4717, 2850, 3123, 1718]  [5, 4067, 4067, 2978]      [5, 2720, 2720, 1999]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Relation Word</th>\n",
       "      <th>True Relation</th>\n",
       "      <th>True Relation Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[15, 5606, 4717, 3123, 5555, 4830]</td>\n",
       "      <td>[5, 1933, 1933, 3179]</td>\n",
       "      <td>[5, 1303, 1303, 2132]</td>\n",
       "      <td>[5, 3701, 2696, 1544]</td>\n",
       "      <td>[5, 2507, 1812, 2188, 614]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[15, 5606, 4717, 3123, 5555, 4830]</td>\n",
       "      <td>[5, 3701, 2696, 1544]</td>\n",
       "      <td>[5, 2507, 1812, 2188, 614]</td>\n",
       "      <td>[5, 3701, 2696, 1544]</td>\n",
       "      <td>[5, 2507, 1812, 2188, 614]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[2332, 5606, 3123, 4075]</td>\n",
       "      <td>[5, 2239, 4484, 1962]</td>\n",
       "      <td>[5, 1515, 1730, 1515, 985, 2438, 2051]</td>\n",
       "      <td>[5, 914, 4342, 3113]</td>\n",
       "      <td>[5, 624, 2891, 2659, 1384, 696]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[2332, 5606, 3123, 4075]</td>\n",
       "      <td>[5, 914, 4342, 289]</td>\n",
       "      <td>[5, 624, 2891, 174]</td>\n",
       "      <td>[5, 914, 4342, 3113]</td>\n",
       "      <td>[5, 624, 2891, 2659, 1384, 696]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[2332, 5606, 3123, 4075]</td>\n",
       "      <td>[5, 914, 4342, 3153]</td>\n",
       "      <td>[5, 624, 2891, 2111]</td>\n",
       "      <td>[5, 914, 4342, 3113]</td>\n",
       "      <td>[5, 624, 2891, 2659, 1384, 696]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Example ID                            Question               Relation                           Relation Word          True Relation               True Relation Word\n",
       "0           0  [15, 5606, 4717, 3123, 5555, 4830]  [5, 1933, 1933, 3179]                   [5, 1303, 1303, 2132]  [5, 3701, 2696, 1544]       [5, 2507, 1812, 2188, 614]\n",
       "1           0  [15, 5606, 4717, 3123, 5555, 4830]  [5, 3701, 2696, 1544]              [5, 2507, 1812, 2188, 614]  [5, 3701, 2696, 1544]       [5, 2507, 1812, 2188, 614]\n",
       "2           1            [2332, 5606, 3123, 4075]  [5, 2239, 4484, 1962]  [5, 1515, 1730, 1515, 985, 2438, 2051]   [5, 914, 4342, 3113]  [5, 624, 2891, 2659, 1384, 696]\n",
       "3           1            [2332, 5606, 3123, 4075]    [5, 914, 4342, 289]                     [5, 624, 2891, 174]   [5, 914, 4342, 3113]  [5, 624, 2891, 2659, 1384, 696]\n",
       "4           1            [2332, 5606, 3123, 4075]   [5, 914, 4342, 3153]                    [5, 624, 2891, 2111]   [5, 914, 4342, 3113]  [5, 624, 2891, 2659, 1384, 696]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "from lib.text_encoders import StaticTokenizerEncoder\n",
    "from lib.text_encoders import DelimiterEncoder\n",
    "from lib.text_encoders import WordEncoder\n",
    "\n",
    "# We add development dataset to text_encoder for embeddings\n",
    "# We make sure not to use the the development dataset to provide us with any vocab optimizations or learning\n",
    "text_encoder = WordEncoder(train_dataset['Question'] + dev_dataset['Question'], lower=True, append_eos=False)\n",
    "print('Text encoder vocab size: %d' % text_encoder.vocab_size)\n",
    "\n",
    "relations = set(train_dataset['True Relation'] + train_dataset['False Relation'])\n",
    "relation_word_encoder = StaticTokenizerEncoder(relations, tokenize=lambda s: re.split('/|_', s))\n",
    "print('Relation word encoder vocab size: %d' % relation_word_encoder.vocab_size)\n",
    "\n",
    "relation_encoder = DelimiterEncoder('/', relations)\n",
    "print('Relation encoder vocab size: %d' % relation_encoder.vocab_size)\n",
    "\n",
    "for dataset in [train_dataset, dev_dataset]:\n",
    "    for row in dataset:\n",
    "        row['Question'] = text_encoder.encode(row['Question'])\n",
    "        row['True Relation Word'] = relation_word_encoder.encode(row['True Relation'])\n",
    "        row['True Relation'] = relation_encoder.encode(row['True Relation'])\n",
    "        \n",
    "        if 'False Relation' in row:\n",
    "            row['False Relation Word'] = relation_word_encoder.encode(row['False Relation'])\n",
    "            row['False Relation'] = relation_encoder.encode(row['False Relation'])\n",
    "\n",
    "        if 'Relation' in row:\n",
    "            row['Relation Word'] = relation_word_encoder.encode(row['Relation'])\n",
    "            row['Relation'] = relation_encoder.encode(row['Relation'])\n",
    "            \n",
    "\n",
    "print('Train Sample:')\n",
    "display(pd.DataFrame(train_dataset[:5]))\n",
    "print('Development Sample:')\n",
    "display(pd.DataFrame(dev_dataset[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Iterators\n",
    "\n",
    "Define functions to create iterators over the development and the train dataset for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lib.utils import pad_batch\n",
    "from lib.samplers import BucketBatchSampler\n",
    "from lib.samplers import SortedSampler\n",
    "\n",
    "\n",
    "train_max_batch_size = 2\n",
    "dev_max_batch_size = 128\n",
    "\n",
    "# Defines how to combine a batch of rows into a tensor\n",
    "def collate_fn(batch, train=True):\n",
    "    \"\"\" list of tensors to a batch variable \"\"\"\n",
    "    question_batch, _ = pad_batch([row['Question'] for row in batch])\n",
    "    \n",
    "    # PyTorch RNN requires batches to be transposed for speed and integration with CUDA\n",
    "    to_variable = (\n",
    "        lambda b: Variable(torch.stack(b).t_().squeeze(0).contiguous(), volatile=not train))\n",
    "\n",
    "    if train:\n",
    "        true_relation_word_batch, _ = pad_batch([row['True Relation Word'] for row in batch])\n",
    "        true_relation_batch, _ = pad_batch([row['True Relation'] for row in batch])\n",
    "        false_relation_word_batch, _ = pad_batch([row['False Relation Word'] for row in batch])\n",
    "        false_relation_batch, _ = pad_batch([row['False Relation'] for row in batch])\n",
    "        return (to_variable(question_batch),\n",
    "                to_variable(true_relation_batch),\n",
    "                to_variable(true_relation_word_batch), \n",
    "                to_variable(false_relation_batch),\n",
    "                to_variable(false_relation_word_batch))\n",
    "    else:\n",
    "        relation_word_batch, _ = pad_batch([row['Relation Word'] for row in batch])\n",
    "        relation_batch, _ = pad_batch([row['Relation'] for row in batch])\n",
    "        return (to_variable(question_batch),\n",
    "                to_variable(relation_batch),\n",
    "                to_variable(relation_word_batch),\n",
    "                batch)\n",
    "\n",
    "\n",
    "def make_train_iterator():\n",
    "    # Use bucket sampling to group similar sized text but with noise + random\n",
    "    sort_key = lambda r: r['Question'].size()[0]\n",
    "    batch_sampler = BucketBatchSampler(train_dataset, sort_key, train_max_batch_size)\n",
    "    return DataLoader(\n",
    "        train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=is_cuda,\n",
    "        num_workers=0)\n",
    "\n",
    "\n",
    "def make_dev_iterator():\n",
    "    # Group together all examples for metrics and sort questions of similar sizes for speed\n",
    "    sort_key = lambda r: (r['Question'].size()[0], r['Example ID'])\n",
    "    return DataLoader(\n",
    "        dev_dataset,\n",
    "        batch_size=dev_max_batch_size,\n",
    "        sampler=SortedSampler(dev_dataset, sort_key, sort_noise=0.0),\n",
    "        collate_fn=partial(collate_fn, train=False),\n",
    "        pin_memory=is_cuda,\n",
    "        num_workers=0)\n",
    "\n",
    "# Just to make sure everything runs\n",
    "train_iterator_test = make_train_iterator()\n",
    "dev_iterator_test = make_dev_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "Instantiate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import MarginRankingLoss\n",
    "\n",
    "# QUESTION: Is there a better margin? or wrose?\n",
    "criterion = cuda(MarginRankingLoss(margin=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from lib.text_encoders import PADDING_INDEX\n",
    "\n",
    "class YuModel(nn.Module):\n",
    "\n",
    "    def __init__(self, relation_vocab_size, relation_word_vocab_size, text_vocab_size,\n",
    "                 embedding_size=300, hidden_size=200):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.relation_embedding = nn.Embedding(\n",
    "            relation_vocab_size, embedding_size, padding_idx=PADDING_INDEX)\n",
    "        self.relation_word_embedding = nn.Embedding(\n",
    "            relation_word_vocab_size, embedding_size, padding_idx=PADDING_INDEX)\n",
    "        self.relation_word_rnn = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            bidirectional=True)\n",
    "        self.relation_rnn = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            bidirectional=True)\n",
    "        \n",
    "        self.text_embedding = nn.Embedding(\n",
    "            text_vocab_size, embedding_size, padding_idx=PADDING_INDEX)\n",
    "        self.text_embedding.weight.requires_grad = False\n",
    "        self.text_rnn = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=0,\n",
    "            bidirectional=True)\n",
    "        \n",
    "        self.distance = nn.CosineSimilarity(dim=1)\n",
    "        \n",
    "    def forward(self, text, relation, relation_word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text: (torch.LongTensor [textn_len, batch_size])\n",
    "            relation: (torch.LongTensor [relation_len, batch_size])\n",
    "            relation_word: (torch.LongTensor [relation_word_len, batch_size])\n",
    "        \"\"\"\n",
    "        batch_size = text.size()[1]\n",
    "        \n",
    "        relation_word = self.relation_word_embedding(relation_word)\n",
    "        _, (relation_word, _) = self.relation_word_rnn(relation_word)\n",
    "        relation_word = relation_word[-2:].transpose(0, 1).contiguous().view(batch_size, -1)\n",
    "        \n",
    "        relation = self.relation_embedding(relation)\n",
    "        _, (relation, _) = self.relation_rnn(relation)\n",
    "        relation = relation[-2:].transpose(0, 1).contiguous().view(batch_size, -1)\n",
    "        \n",
    "        # (2, batch_size, hidden_size * 2)\n",
    "        relation = torch.stack([relation, relation_word])\n",
    "        # (batch_size, hidden_size * 2)\n",
    "        relation = torch.max(relation, dim=0)[0]\n",
    "        print(relation.size())\n",
    "        \n",
    "        print(text.size())\n",
    "        text = self.text_embedding(text)\n",
    "        print(text.size())\n",
    "        _, (text, _) = self.text_rnn(text)\n",
    "        text = text[-2:].transpose(0, 1).contiguous().view(batch_size, -1)\n",
    "        print(text.size())\n",
    "        \n",
    "        # (batch_size)\n",
    "        return self.distance(text, relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.pretrained_embeddings import FastText\n",
    "\n",
    "# Load embeddings\n",
    "unk_init = lambda t: torch.FloatTensor(t).uniform_(-0.1, 0.1)\n",
    "pretrained_embedding = FastText(language='en', cache='./../../.pretrained_embeddings_cache')\n",
    "embedding_weights = torch.Tensor(text_encoder.vocab_size, pretrained_embedding.dim)\n",
    "for i, token in enumerate(text_encoder.vocab):\n",
    "    embedding_weights[i] = pretrained_embedding[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.nn import SeqToLabel\n",
    "\n",
    "def make_model():\n",
    "    model = YuModel(relation_word_encoder.vocab_size, relation_encoder.vocab_size, text_encoder.vocab_size)\n",
    "    for param in model.parameters():\n",
    "        param.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    model.text_embedding.weight.data.copy_(embedding_weights)\n",
    "\n",
    "    cuda(model)\n",
    "    return model\n",
    "\n",
    "# Test that making the model works\n",
    "model_test = make_model()\n",
    "model_test = None # Clear memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Optimizer \n",
    "\n",
    "Instantiate the gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "from lib.optim import Optimizer\n",
    "\n",
    "# https://github.com/pytorch/pytorch/issues/679\n",
    "# TODO: Try SGD\n",
    "def make_optimizer(model):\n",
    "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = Optimizer(Adam(params=params))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Below here, we do a training loop over a number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Directory: logs/0000.12-31_14:16:42.yu_relation_model\n",
      "Devevelopment Max Batch Size: 128\n",
      "Train Max Batch Size: 2\n",
      "Epochs: 30\n",
      "Total Parameters: 5645600\n",
      "Model:\n",
      "YuModel (\n",
      "  (relation_embedding): Embedding(3022, 300, padding_idx=0)\n",
      "  (relation_word_embedding): Embedding(4554, 300, padding_idx=0)\n",
      "  (relation_word_rnn): LSTM(300, 200, bidirectional=True)\n",
      "  (relation_rnn): LSTM(300, 200, bidirectional=True)\n",
      "  (text_embedding): Embedding(5947, 300, padding_idx=0)\n",
      "  (text_rnn): LSTM(300, 200, num_layers=2, bidirectional=True)\n",
      "  (distance): CosineSimilarity (\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from lib.utils import get_total_parameters\n",
    "from lib.utils import get_log_directory_path\n",
    "\n",
    "epochs = 30\n",
    "log_directory = get_log_directory_path('yu_relation_model')\n",
    "model = make_model()\n",
    "optimizer = make_optimizer(model)\n",
    "\n",
    "print('Log Directory: %s' % log_directory)\n",
    "print('Devevelopment Max Batch Size: %s' % dev_max_batch_size)\n",
    "print('Train Max Batch Size: %s' % train_max_batch_size)\n",
    "print('Epochs: %s' % epochs)\n",
    "print('Total Parameters: %d' % get_total_parameters(model))\n",
    "print('Model:\\n%s' % model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d978223332574b959ab982bf79373efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=249519), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400])\n",
      "torch.Size([35, 1])\n",
      "torch.Size([35, 1, 300])\n",
      "torch.Size([1, 400])\n",
      "torch.Size([1, 400])\n",
      "torch.Size([35, 1])\n",
      "torch.Size([35, 1, 300])\n",
      "torch.Size([1, 400])\n",
      "torch.Size([2, 400])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([4, 2, 300])\n",
      "torch.Size([2, 400])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/loss.py:140: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  _output.mul_(-1).mul_(y)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/loss.py:158: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  dist.mul_(-1).mul_(y)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/loss.py:163: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  grad_input1.mul_(-1).mul_(y)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/loss.py:165: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  grad_input2.mul_(y)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/torch/lib/THC/generated/../generic/THCTensorMathPointwise.cu:174",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2fd6e6e80045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m          false_relation, false_relation_word) in tqdm_notebook(train_iterator):\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_relation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_relation_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moutput_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_relation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_relation_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-13c20f30e432>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, relation, relation_word)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# (batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/distance.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(x1, x2, dim, eps)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \"\"\"\n\u001b[1;32m   1100\u001b[0m     \u001b[0mw12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m     \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m     \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw12\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mNorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/_functions/reduce.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, p, dim, keepdim)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/torch/lib/THC/generated/../generic/THCTensorMathPointwise.cu:174"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "\n",
    "from lib.metrics import get_accuracy\n",
    "from lib.metrics import get_accuracy_top_k\n",
    "from lib.metrics import print_random_sample\n",
    "from lib.checkpoint import Checkpoint\n",
    "\n",
    "# Train!\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch %d' % epoch)\n",
    "    \n",
    "    # Iterate over the training data\n",
    "    model.train(mode=True)\n",
    "    train_iterator = make_train_iterator()\n",
    "    for (question, true_relation, true_relation_word,\n",
    "         false_relation, false_relation_word) in tqdm_notebook(train_iterator):\n",
    "        optimizer.zero_grad()\n",
    "        output_true = model(cuda_async(question), cuda_async(true_relation), cuda_async(true_relation_word))\n",
    "        output_false = model(cuda_async(question), cuda_async(false_relation), cuda_async(false_relation_word))\n",
    "        labels = cuda(Variable(torch.ones(1, output_true.size()[0])))\n",
    "        loss = criterion(output_true, output_false, labels)\n",
    "\n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    Checkpoint.save(\n",
    "        log_directory,\n",
    "        {\n",
    "            'model': model,\n",
    "            'optimizer': optimizer,\n",
    "            'relation_word_encoder': relation_word_encoder,\n",
    "            'relation_encoder': relation_encoder,\n",
    "            'text_encoder': text_encoder\n",
    "        },\n",
    "        device=device)\n",
    "\n",
    "    # Evaluate\n",
    "    model.train(mode=False)\n",
    "    examples = defaultdict(list)\n",
    "    dev_iterator = make_dev_iterator()\n",
    "    total_loss = 0\n",
    "    for (question, relation, relation_word, batch) in tqdm_notebook(dev_iterator):\n",
    "        output = model(cuda_async(question), cuda_async(relation), cuda_async(relation_word))\n",
    "        output = output.data.cpu()\n",
    "        \n",
    "        for i, row in enumerate(batch):                         \n",
    "            examples[row['Example ID']].append({\n",
    "                'Score': output[i],\n",
    "                'Question': row['Question'],\n",
    "                'True Relation': row['True Relation'],\n",
    "                'Relation': row['Relation']\n",
    "            })\n",
    "\n",
    "    # Print metrics\n",
    "    correct = 0\n",
    "    for pool in examples.values():\n",
    "        max_relation = max(pool, key=lambda p: p['Score'])\n",
    "        if max_relation['Relation'] == max_relation['True Relation']:\n",
    "            correct += 1\n",
    "    print('Accuracy: %f [%d of %d]' % (correct / len(examples), correct, len(examples)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
