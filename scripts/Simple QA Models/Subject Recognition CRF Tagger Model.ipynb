{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject Recognition CRF Tagger Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../../allennlp')\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from allennlp.common.params import Params\n",
    "\n",
    "crf_tagger = {\n",
    "    \"dataset_reader\": {\n",
    "        \"type\": \"sequence_tagging\",\n",
    "        \"word_tag_delimiter\": \"/\",\n",
    "        \"token_indexers\": {\n",
    "            \"tokens\": {\n",
    "                \"type\": \"single_id\",\n",
    "                \"lowercase_tokens\": True\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"characters\",\n",
    "                \"character_tokenizer\": {\n",
    "                    \"end_tokens\": [\n",
    "                        \"@@PADDING@@\",\n",
    "                        \"@@PADDING@@\",\n",
    "                        \"@@PADDING@@\",\n",
    "                        \"@@PADDING@@\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # TODO: Update this to the location for subject name recognition training data\n",
    "    \"train_data_path\": './../../data/subject_recognition/train.txt', \n",
    "    \"validation_data_path\": './../../data/subject_recognition/dev.txt',\n",
    "    \"model\": {\n",
    "        \"type\": \"crf_tagger\",\n",
    "        \"text_field_embedder\": {\n",
    "            \"tokens\": {\n",
    "                \"type\": \"embedding\",\n",
    "                \"pretrained_file\": \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\",\n",
    "                \"embedding_dim\": 100,\n",
    "                \"trainable\": False\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 16\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"cnn\",\n",
    "                    \"embedding_dim\": 16,\n",
    "                    \"num_filters\": 100,\n",
    "                    \"ngram_filter_sizes\": [\n",
    "                        5\n",
    "                    ]\n",
    "                },\n",
    "                \"dropout\": 0.6\n",
    "            }\n",
    "        },\n",
    "        \"encoder\": {\n",
    "            \"type\": \"lstm\",\n",
    "            \"input_size\": 200,\n",
    "            \"hidden_size\": 300,\n",
    "            \"num_layers\": 3,\n",
    "            \"dropout\": 0.2,\n",
    "            \"bidirectional\": True\n",
    "        }\n",
    "    },\n",
    "    \"iterator\": {\n",
    "        \"type\": \"bucket\",\n",
    "        \"sorting_keys\": [\n",
    "            [\n",
    "                \"tokens\",\n",
    "                \"num_tokens\"\n",
    "            ]\n",
    "        ],\n",
    "        \"batch_size\": 16\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"num_epochs\": 10,\n",
    "        \"grad_norm\": 1.0,\n",
    "        \"patience\": 3,\n",
    "        \"cuda_device\": 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialization Directory: ./../../logs/0000.01-15_21:34:34.subject_recognition_crf_tagger\n",
      "2018-01-15 21:34:34,010 | INFO : random_seed = 13370\n",
      "2018-01-15 21:34:34,011 | INFO : numpy_seed = 1337\n",
      "2018-01-15 21:34:34,011 | INFO : pytorch_seed = 133\n",
      "2018-01-15 21:34:34,013 | INFO : Pytorch version: 0.3.0.post4\n",
      "2018-01-15 21:34:34,015 | INFO : dataset_reader.type = sequence_tagging\n",
      "2018-01-15 21:34:34,016 | INFO : dataset_reader.token_indexers.tokens.type = single_id\n",
      "2018-01-15 21:34:34,016 | INFO : dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "2018-01-15 21:34:34,016 | INFO : dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "2018-01-15 21:34:34,017 | INFO : dataset_reader.token_indexers.token_characters.type = characters\n",
      "2018-01-15 21:34:34,017 | INFO : dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "2018-01-15 21:34:34,018 | INFO : dataset_reader.token_indexers.token_characters.character_tokenizer.byte_encoding = None\n",
      "2018-01-15 21:34:34,018 | INFO : dataset_reader.token_indexers.token_characters.character_tokenizer.lowercase_characters = False\n",
      "2018-01-15 21:34:34,019 | INFO : dataset_reader.token_indexers.token_characters.character_tokenizer.start_tokens = None\n",
      "2018-01-15 21:34:34,019 | INFO : dataset_reader.token_indexers.token_characters.character_tokenizer.end_tokens = ['@@PADDING@@', '@@PADDING@@', '@@PADDING@@', '@@PADDING@@']\n",
      "2018-01-15 21:34:34,020 | INFO : dataset_reader.word_tag_delimiter = /\n",
      "2018-01-15 21:34:34,020 | INFO : dataset_reader.token_delimiter = None\n",
      "2018-01-15 21:34:34,021 | INFO : train_data_path = ./../../data/subject_recognition/train.txt\n",
      "2018-01-15 21:34:34,021 | INFO : Reading training data from ./../../data/subject_recognition/train.txt\n",
      "2018-01-15 21:34:34,022 | INFO : Reading instances from lines in file at: ./../../data/subject_recognition/train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74520it [00:02, 33110.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:34:36,338 | INFO : validation_data_path = ./../../data/subject_recognition/dev.txt\n",
      "2018-01-15 21:34:36,339 | INFO : Reading validation data from ./../../data/subject_recognition/dev.txt\n",
      "2018-01-15 21:34:36,339 | INFO : Reading instances from lines in file at: ./../../data/subject_recognition/dev.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10648it [00:00, 20577.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:34:36,868 | INFO : test_data_path = None\n",
      "2018-01-15 21:34:36,869 | INFO : Creating a vocabulary using validation, train data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:34:36,962 | INFO : vocabulary.directory_path = None\n",
      "2018-01-15 21:34:36,963 | INFO : vocabulary.min_count = 1\n",
      "2018-01-15 21:34:36,964 | INFO : vocabulary.max_vocab_size = None\n",
      "2018-01-15 21:34:36,966 | INFO : vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
      "2018-01-15 21:34:36,966 | INFO : vocabulary.only_include_pretrained_words = False\n",
      "2018-01-15 21:34:36,966 | INFO : Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 85168/85168 [00:06<00:00, 12858.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:34:43,766 | INFO : model.type = crf_tagger\n",
      "2018-01-15 21:34:43,768 | INFO : model.text_field_embedder.type = basic\n",
      "2018-01-15 21:34:43,769 | INFO : model.text_field_embedder.tokens.type = embedding\n",
      "2018-01-15 21:34:43,769 | INFO : model.text_field_embedder.tokens.num_embeddings = None\n",
      "2018-01-15 21:34:43,770 | INFO : model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "2018-01-15 21:34:43,770 | INFO : model.text_field_embedder.tokens.embedding_dim = 100\n",
      "2018-01-15 21:34:43,771 | INFO : model.text_field_embedder.tokens.pretrained_file = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\n",
      "2018-01-15 21:34:43,771 | INFO : model.text_field_embedder.tokens.projection_dim = None\n",
      "2018-01-15 21:34:43,772 | INFO : model.text_field_embedder.tokens.trainable = False\n",
      "2018-01-15 21:34:43,772 | INFO : model.text_field_embedder.tokens.padding_index = None\n",
      "2018-01-15 21:34:43,772 | INFO : model.text_field_embedder.tokens.max_norm = None\n",
      "2018-01-15 21:34:43,773 | INFO : model.text_field_embedder.tokens.norm_type = 2.0\n",
      "2018-01-15 21:34:43,773 | INFO : model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "2018-01-15 21:34:43,774 | INFO : model.text_field_embedder.tokens.sparse = False\n",
      "2018-01-15 21:34:43,776 | INFO : Reading embeddings from file\n",
      "2018-01-15 21:34:49,071 | INFO : Initializing pre-trained embedding layer\n",
      "2018-01-15 21:34:49,509 | INFO : model.text_field_embedder.token_characters.type = character_encoding\n",
      "2018-01-15 21:34:49,510 | INFO : model.text_field_embedder.token_characters.embedding.num_embeddings = None\n",
      "2018-01-15 21:34:49,511 | INFO : model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "2018-01-15 21:34:49,511 | INFO : model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "2018-01-15 21:34:49,512 | INFO : model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "2018-01-15 21:34:49,512 | INFO : model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "2018-01-15 21:34:49,512 | INFO : model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "2018-01-15 21:34:49,513 | INFO : model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "2018-01-15 21:34:49,514 | INFO : model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "2018-01-15 21:34:49,515 | INFO : model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "2018-01-15 21:34:49,515 | INFO : model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "2018-01-15 21:34:49,516 | INFO : model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "2018-01-15 21:34:49,517 | INFO : model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "2018-01-15 21:34:49,517 | INFO : model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "2018-01-15 21:34:49,518 | INFO : model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "2018-01-15 21:34:49,518 | INFO : model.text_field_embedder.token_characters.encoder.num_filters = 100\n",
      "2018-01-15 21:34:49,519 | INFO : model.text_field_embedder.token_characters.encoder.conv_layer_activation = relu\n",
      "2018-01-15 21:34:49,519 | INFO : model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [5]\n",
      "2018-01-15 21:34:49,521 | INFO : model.text_field_embedder.token_characters.dropout = 0.6\n",
      "2018-01-15 21:34:49,521 | INFO : model.encoder.type = lstm\n",
      "2018-01-15 21:34:49,522 | INFO : model.encoder.batch_first = True\n",
      "2018-01-15 21:34:49,522 | INFO : Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2018-01-15 21:34:49,523 | INFO : CURRENTLY DEFINED PARAMETERS: \n",
      "2018-01-15 21:34:49,523 | INFO : model.encoder.input_size = 200\n",
      "2018-01-15 21:34:49,523 | INFO : model.encoder.hidden_size = 300\n",
      "2018-01-15 21:34:49,524 | INFO : model.encoder.num_layers = 3\n",
      "2018-01-15 21:34:49,524 | INFO : model.encoder.dropout = 0.2\n",
      "2018-01-15 21:34:49,525 | INFO : model.encoder.bidirectional = True\n",
      "2018-01-15 21:34:49,525 | INFO : model.encoder.batch_first = True\n",
      "2018-01-15 21:34:49,553 | INFO : model.label_namespace = labels\n",
      "2018-01-15 21:34:49,554 | INFO : model.initializer = []\n",
      "2018-01-15 21:34:49,554 | INFO : model.regularizer = []\n",
      "2018-01-15 21:34:49,555 | INFO : Initializing parameters\n",
      "2018-01-15 21:34:49,556 | INFO : Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2018-01-15 21:34:49,556 | INFO :    crf.end_transitions\n",
      "2018-01-15 21:34:49,557 | INFO :    crf.start_transitions\n",
      "2018-01-15 21:34:49,557 | INFO :    crf.transitions\n",
      "2018-01-15 21:34:49,557 | INFO :    encoder._module.bias_hh_l0\n",
      "2018-01-15 21:34:49,558 | INFO :    encoder._module.bias_hh_l0_reverse\n",
      "2018-01-15 21:34:49,558 | INFO :    encoder._module.bias_hh_l1\n",
      "2018-01-15 21:34:49,559 | INFO :    encoder._module.bias_hh_l1_reverse\n",
      "2018-01-15 21:34:49,559 | INFO :    encoder._module.bias_hh_l2\n",
      "2018-01-15 21:34:49,560 | INFO :    encoder._module.bias_hh_l2_reverse\n",
      "2018-01-15 21:34:49,560 | INFO :    encoder._module.bias_ih_l0\n",
      "2018-01-15 21:34:49,561 | INFO :    encoder._module.bias_ih_l0_reverse\n",
      "2018-01-15 21:34:49,561 | INFO :    encoder._module.bias_ih_l1\n",
      "2018-01-15 21:34:49,562 | INFO :    encoder._module.bias_ih_l1_reverse\n",
      "2018-01-15 21:34:49,562 | INFO :    encoder._module.bias_ih_l2\n",
      "2018-01-15 21:34:49,562 | INFO :    encoder._module.bias_ih_l2_reverse\n",
      "2018-01-15 21:34:49,563 | INFO :    encoder._module.weight_hh_l0\n",
      "2018-01-15 21:34:49,563 | INFO :    encoder._module.weight_hh_l0_reverse\n",
      "2018-01-15 21:34:49,564 | INFO :    encoder._module.weight_hh_l1\n",
      "2018-01-15 21:34:49,564 | INFO :    encoder._module.weight_hh_l1_reverse\n",
      "2018-01-15 21:34:49,565 | INFO :    encoder._module.weight_hh_l2\n",
      "2018-01-15 21:34:49,565 | INFO :    encoder._module.weight_hh_l2_reverse\n",
      "2018-01-15 21:34:49,566 | INFO :    encoder._module.weight_ih_l0\n",
      "2018-01-15 21:34:49,566 | INFO :    encoder._module.weight_ih_l0_reverse\n",
      "2018-01-15 21:34:49,566 | INFO :    encoder._module.weight_ih_l1\n",
      "2018-01-15 21:34:49,567 | INFO :    encoder._module.weight_ih_l1_reverse\n",
      "2018-01-15 21:34:49,567 | INFO :    encoder._module.weight_ih_l2\n",
      "2018-01-15 21:34:49,568 | INFO :    encoder._module.weight_ih_l2_reverse\n",
      "2018-01-15 21:34:49,568 | INFO :    tag_projection_layer._module.bias\n",
      "2018-01-15 21:34:49,569 | INFO :    tag_projection_layer._module.weight\n",
      "2018-01-15 21:34:49,569 | INFO :    text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "2018-01-15 21:34:49,570 | INFO :    text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "2018-01-15 21:34:49,571 | INFO :    text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "2018-01-15 21:34:49,571 | INFO :    text_field_embedder.token_embedder_tokens.weight\n",
      "2018-01-15 21:34:49,572 | INFO : iterator.type = bucket\n",
      "2018-01-15 21:34:49,572 | INFO : iterator.sorting_keys = [['tokens', 'num_tokens']]\n",
      "2018-01-15 21:34:49,573 | INFO : iterator.padding_noise = 0.1\n",
      "2018-01-15 21:34:49,573 | INFO : iterator.biggest_batch_first = False\n",
      "2018-01-15 21:34:49,574 | INFO : iterator.batch_size = 16\n",
      "2018-01-15 21:34:49,574 | INFO : Indexing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 74520/74520 [00:09<00:00, 7767.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:34:59,170 | INFO : Indexing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########| 10648/10648 [00:01<00:00, 8253.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:35:00,462 | INFO : trainer.patience = 3\n",
      "2018-01-15 21:35:00,463 | INFO : trainer.validation_metric = -loss\n",
      "2018-01-15 21:35:00,464 | INFO : trainer.num_epochs = 10\n",
      "2018-01-15 21:35:00,465 | INFO : trainer.cuda_device = 0\n",
      "2018-01-15 21:35:00,466 | INFO : trainer.grad_norm = 1.0\n",
      "2018-01-15 21:35:00,467 | INFO : trainer.grad_clipping = None\n",
      "2018-01-15 21:35:00,468 | INFO : trainer.learning_rate_scheduler = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:35:02,521 | INFO : trainer.optimizer = adam\n",
      "2018-01-15 21:35:02,522 | INFO : Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2018-01-15 21:35:02,524 | INFO : CURRENTLY DEFINED PARAMETERS: \n",
      "2018-01-15 21:35:02,525 | INFO : trainer.no_tqdm = False\n",
      "2018-01-15 21:35:02,530 | INFO : evaluate_on_test = False\n",
      "2018-01-15 21:35:02,533 | INFO : Beginning training.\n",
      "2018-01-15 21:35:02,533 | INFO : Epoch 0/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:35:02,535 | INFO : Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "token_accuracy: 0.96, accuracy: 0.85, loss: 12.45 ||: 100%|##########| 4658/4658 [01:56<00:00, 40.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:36:58,562 | INFO : Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "token_accuracy: 0.98, accuracy: 0.92, loss: 5.83 ||: 100%|##########| 666/666 [00:09<00:00, 73.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:37:07,684 | INFO : Best validation performance so far. Copying weights to './../../logs/0000.01-15_21:34:34.subject_recognition_crf_tagger/best.th'.\n",
      "2018-01-15 21:37:07,719 | INFO : Training token_accuracy : 0.963872    Validation token_accuracy : 0.976746 \n",
      "2018-01-15 21:37:07,721 | INFO : Training accuracy : 0.848846    Validation accuracy : 0.918107 \n",
      "2018-01-15 21:37:07,723 | INFO : Training loss : 12.448190    Validation loss : 5.828502 \n",
      "2018-01-15 21:37:07,725 | INFO : Epoch duration: 00:02:05\n",
      "2018-01-15 21:37:07,727 | INFO : Estimated training time remaining: 00:18:46\n",
      "2018-01-15 21:37:07,730 | INFO : Epoch 1/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:37:07,733 | INFO : Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "token_accuracy: 0.98, accuracy: 0.93, loss: 5.37 ||: 100%|##########| 4658/4658 [01:55<00:00, 40.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:39:03,531 | INFO : Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "token_accuracy: 0.97, accuracy: 0.94, loss: 5.22 ||: 100%|##########| 666/666 [00:08<00:00, 74.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:39:12,567 | INFO : Best validation performance so far. Copying weights to './../../logs/0000.01-15_21:34:34.subject_recognition_crf_tagger/best.th'.\n",
      "2018-01-15 21:39:12,639 | INFO : Training token_accuracy : 0.975723    Validation token_accuracy : 0.969536 \n",
      "2018-01-15 21:39:12,641 | INFO : Training accuracy : 0.930287    Validation accuracy : 0.937547 \n",
      "2018-01-15 21:39:12,647 | INFO : Training loss : 5.374257    Validation loss : 5.222215 \n",
      "2018-01-15 21:39:12,648 | INFO : Epoch duration: 00:02:04\n",
      "2018-01-15 21:39:12,651 | INFO : Estimated training time remaining: 00:16:40\n",
      "2018-01-15 21:39:12,652 | INFO : Epoch 2/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:39:12,655 | INFO : Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "token_accuracy: 0.97, accuracy: 0.95, loss: 4.13 ||: 100%|##########| 4658/4658 [01:55<00:00, 40.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:41:08,290 | INFO : Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "token_accuracy: 0.97, accuracy: 0.94, loss: 5.18 ||: 100%|##########| 666/666 [00:09<00:00, 73.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:41:17,451 | INFO : Best validation performance so far. Copying weights to './../../logs/0000.01-15_21:34:34.subject_recognition_crf_tagger/best.th'.\n",
      "2018-01-15 21:41:17,551 | INFO : Training token_accuracy : 0.973081    Validation token_accuracy : 0.970676 \n",
      "2018-01-15 21:41:17,555 | INFO : Training accuracy : 0.947330    Validation accuracy : 0.941773 \n",
      "2018-01-15 21:41:17,557 | INFO : Training loss : 4.133083    Validation loss : 5.181502 \n",
      "2018-01-15 21:41:17,558 | INFO : Epoch duration: 00:02:04\n",
      "2018-01-15 21:41:17,560 | INFO : Estimated training time remaining: 00:14:35\n",
      "2018-01-15 21:41:17,563 | INFO : Epoch 3/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:41:17,567 | INFO : Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "token_accuracy: 0.97, accuracy: 0.96, loss: 3.32 ||: 100%|##########| 4658/4658 [01:55<00:00, 40.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:43:13,461 | INFO : Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "token_accuracy: 0.97, accuracy: 0.95, loss: 5.34 ||: 100%|##########| 666/666 [00:09<00:00, 73.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:43:22,549 | INFO : Training token_accuracy : 0.973321    Validation token_accuracy : 0.974707 \n",
      "2018-01-15 21:43:22,551 | INFO : Training accuracy : 0.958468    Validation accuracy : 0.948347 \n",
      "2018-01-15 21:43:22,552 | INFO : Training loss : 3.323836    Validation loss : 5.343469 \n",
      "2018-01-15 21:43:22,552 | INFO : Epoch duration: 00:02:04\n",
      "2018-01-15 21:43:22,553 | INFO : Estimated training time remaining: 00:12:30\n",
      "2018-01-15 21:43:22,553 | INFO : Epoch 4/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:43:22,554 | INFO : Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "token_accuracy: 0.98, accuracy: 0.97, loss: 2.76 ||: 100%|##########| 4658/4658 [01:54<00:00, 40.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:45:17,468 | INFO : Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "token_accuracy: 0.97, accuracy: 0.95, loss: 5.50 ||: 100%|##########| 666/666 [00:09<00:00, 70.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:45:27,050 | INFO : Training token_accuracy : 0.975615    Validation token_accuracy : 0.968350 \n",
      "2018-01-15 21:45:27,053 | INFO : Training accuracy : 0.967351    Validation accuracy : 0.947690 \n",
      "2018-01-15 21:45:27,054 | INFO : Training loss : 2.760576    Validation loss : 5.500036 \n",
      "2018-01-15 21:45:27,054 | INFO : Epoch duration: 00:02:04\n",
      "2018-01-15 21:45:27,054 | INFO : Estimated training time remaining: 00:10:24\n",
      "2018-01-15 21:45:27,055 | INFO : Epoch 5/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:45:27,056 | INFO : Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "token_accuracy: 0.98, accuracy: 0.97, loss: 2.21 ||: 100%|##########| 4658/4658 [01:53<00:00, 41.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:47:20,568 | INFO : Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "token_accuracy: 0.98, accuracy: 0.95, loss: 5.90 ||: 100%|##########| 666/666 [00:08<00:00, 74.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-15 21:47:29,499 | INFO : Ran out of patience.  Stopping training.\n",
      "2018-01-15 21:47:29,500 | INFO : archiving weights and vocabulary to ./../../logs/0000.01-15_21:34:34.subject_recognition_crf_tagger/model.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CrfTagger(\n",
       "  (text_field_embedder): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): Embedding(\n",
       "    )\n",
       "    (token_embedder_token_characters): TokenCharactersEncoder(\n",
       "      (_embedding): TimeDistributed(\n",
       "        (_module): Embedding(\n",
       "        )\n",
       "      )\n",
       "      (_encoder): TimeDistributed(\n",
       "        (_module): CnnEncoder(\n",
       "          (_activation): ReLU()\n",
       "          (conv_layer_0): Conv1d (16, 100, kernel_size=(5,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (_dropout): Dropout(p=0.6)\n",
       "    )\n",
       "  )\n",
       "  (encoder): PytorchSeq2SeqWrapper(\n",
       "    (_module): LSTM(200, 300, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (tag_projection_layer): TimeDistributed(\n",
       "    (_module): Linear(in_features=600, out_features=2)\n",
       "  )\n",
       "  (crf): ConditionalRandomField(\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "# Create root logger\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "from allennlp.commands.train import train_model\n",
    "from lib.utils import get_log_directory_path\n",
    "\n",
    "params = Params(crf_tagger)\n",
    "\n",
    "serialization_dir = get_log_directory_path('subject_recognition_crf_tagger', './../../logs')\n",
    "print('Serialization Directory:', serialization_dir)\n",
    "train_model(params=params, serialization_dir=serialization_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
